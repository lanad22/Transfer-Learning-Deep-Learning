{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: AlexNet from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the AlexNet Architecture\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def get_dataloaders(batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root='archive/train', transform=transform)\n",
    "    valid_dataset = datasets.ImageFolder(root='archive/valid', transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root='archive/test', transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, device, num_epochs=10):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        # Training loop with accuracy calculation\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Train Accuracy: {train_accuracy:.2f} %')\n",
    "        \n",
    "        # Validation loop with accuracy calculation\n",
    "        model.eval()\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        running_loss_valid = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss_valid += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "        valid_accuracy = 100 * correct_valid / total_valid\n",
    "        valid_loss = running_loss_valid / len(valid_loader)\n",
    "        print(f'Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.2f} %')\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = get_dataloaders(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet(num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 211/211 [43:46<00:00, 12.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.565894818418964, Train Accuracy: 1.41 %\n",
      "Validation Loss: 4.4309, Validation Accuracy: 1.80 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 211/211 [43:53<00:00, 12.48s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 4.253803169557834, Train Accuracy: 3.48 %\n",
      "Validation Loss: 4.0711, Validation Accuracy: 5.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 211/211 [1:29:21<00:00, 25.41s/batch]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 4.042813511256358, Train Accuracy: 5.54 %\n",
      "Validation Loss: 3.9341, Validation Accuracy: 7.20 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 211/211 [1:49:34<00:00, 31.16s/batch]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 3.8477903180777746, Train Accuracy: 8.57 %\n",
      "Validation Loss: 3.6288, Validation Accuracy: 11.60 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 211/211 [3:32:23<00:00, 60.40s/batch]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 3.620379061495523, Train Accuracy: 12.20 %\n",
      "Validation Loss: 3.5149, Validation Accuracy: 13.40 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 211/211 [47:16<00:00, 13.44s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 3.447340681654582, Train Accuracy: 15.44 %\n",
      "Validation Loss: 3.3517, Validation Accuracy: 16.20 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 211/211 [2:04:47<00:00, 35.49s/batch]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 3.271992040471443, Train Accuracy: 18.62 %\n",
      "Validation Loss: 3.2316, Validation Accuracy: 21.80 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 211/211 [3:17:49<00:00, 56.26s/batch]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 3.1173960043920723, Train Accuracy: 21.92 %\n",
      "Validation Loss: 3.0513, Validation Accuracy: 24.80 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 211/211 [42:20<00:00, 12.04s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 2.9519548054555016, Train Accuracy: 25.32 %\n",
      "Validation Loss: 3.0534, Validation Accuracy: 25.60 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 211/211 [37:52<00:00, 10.77s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.799468681145618, Train Accuracy: 28.38 %\n",
      "Validation Loss: 2.9846, Validation Accuracy: 24.00 %\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 31.20 %\n"
     ]
    }
   ],
   "source": [
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: AlexNet pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning strategy:\n",
    "\n",
    "1. **Modify the Classifier**:\n",
    "    Since AlexNet is pre-trained on ImageNet, which has 1000 classes, we'll adjust the final layer to the number of classes you have in our dataset - 100 classes. \n",
    "    \n",
    "2. **Freeze Feature Layers**: \n",
    "    Freeze the parameters of the feature extraction layers to avoid updating them during training, which can be done by setting `requires_grad = False`.\n",
    "\n",
    "3. **Define the Loss Function and Optimizer**: \n",
    "    Select a loss function Adam, which will only update the parameters of the unfrozen layers.\n",
    "\n",
    "4. **Fine-Tuning**: \n",
    "    Train the model on your new dataset, optionally using techniques such as learning rate decay, data augmentation, or regularization to improve performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Lanado/anaconda3/envs/my_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/Lanado/anaconda3/envs/my_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained AlexNet\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the features layers\n",
    "for param in alexnet.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(alexnet.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.9995, Validation Accuracy: 70.20%\n",
      "Epoch [2/10], Train Loss: 1.2342, Validation Accuracy: 77.00%\n",
      "Epoch [3/10], Train Loss: 1.0467, Validation Accuracy: 74.80%\n",
      "Epoch [4/10], Train Loss: 0.9254, Validation Accuracy: 75.60%\n",
      "Epoch [5/10], Train Loss: 0.8890, Validation Accuracy: 77.60%\n",
      "Epoch [6/10], Train Loss: 0.8696, Validation Accuracy: 77.80%\n",
      "Epoch [7/10], Train Loss: 0.7730, Validation Accuracy: 80.00%\n",
      "Epoch [8/10], Train Loss: 0.7725, Validation Accuracy: 79.40%\n",
      "Epoch [9/10], Train Loss: 0.8670, Validation Accuracy: 77.60%\n",
      "Epoch [10/10], Train Loss: 0.8063, Validation Accuracy: 75.80%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    alexnet.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = alexnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    alexnet.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = alexnet(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "          f'Validation Accuracy: {100 * correct/total:.2f}%')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 79.40 %\n"
     ]
    }
   ],
   "source": [
    "test_model(alexnet, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion \n",
    "\n",
    "**AlexNet from scratch:**\n",
    "- It starts with a high training loss and low accuracy.\n",
    "- There's a consistent improvement in both training loss and accuracy over epochs.\n",
    "- The validation accuracy also improves, but it seems to plateau around 24-25% by the end of 10 epochs.\n",
    "\n",
    "**Pretrained AlexNet:**\n",
    "- It begins with much lower training loss and significantly higher validation accuracy.\n",
    "- There is an improvement in the training loss and validation accuracy initially, but it starts to plateau and slightly fluctuate towards the end of the training.\n",
    "- The validation accuracy is substantially higher, peaking at 80% and staying around the mid to high 70s.\n",
    "\n",
    "\n",
    "The pretrained AlexNet model has a significant advantage in starting from a point where it has already learned a variety of features from a large and diverse dataset (ImageNet). This allows it to adapt to the new task much more quickly. The model trained from scratch is learning to identify features and classify images simultaneously. This is a much more difficult task and explains the slower improvement in performance. The pretrained model only needs to adjust to the specific features of the new dataset.\n",
    "\n",
    "The pretrained model achieves much higher accuracy and lower loss at a much quicker pace, highlighting the benefits of transfer learning, especially when the dataset used for the initial training is large and varied.\n",
    "\n",
    "However, the pretrained model shows signs of overfitting as the validation accuracy starts to decrease while the training loss continues to decrease. This could be due to the model beginning to memorize the training data rather than learning to generalize from it. Regularization techniques, more data, or data augmentation could be used to combat this.\n",
    "\n",
    "Training from scratch is computationally expensive and time-consuming - batch times are significant. Transfer learning is not just more effective in terms of accuracy, it's also more efficient.\n",
    "\n",
    "Lastly, for generalization, the features learned by the model on ImageNet contain a lot of general information that is useful for a wide range of visual recognition tasks. This is why the pretrained model generalizes better to your validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
